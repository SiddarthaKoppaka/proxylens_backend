# ProxyLens RAG Backend (FastAPI + LangGraph + Ollama)

This is the backend for the ThinkWise AI platform — a LangGraph-powered Reasoning Agent that estimates **ROI** and **Implementation Effort** for business ideas using LLMs (like LLaMA 3 via Ollama). It provides an explainable and interactive analysis API.

---

## Features

- ReAct-based LangGraph agent
- LLM-powered reasoning and scoring (via Ollama)
- Returns top 3 business ideas with explanation
- FastAPI REST endpoints
- Dockerized and Kubernetes-ready

---

## API Endpoints

| Endpoint             | Method | Description                            |
|----------------------|--------|----------------------------------------|
| `/analyze/csv`       | POST   | Upload CSV of ideas for analysis       |
| `/analyze/json`      | POST   | Submit ideas as JSON                   |
| `/ideas`             | GET    | Fetch stored ideas                     |
| `/analyze/single`    | POST   | Analyze a single idea (WIP)            |

---

## Local Development

### Install

```bash
pip install -r requirements.txt
````

### Run

```bash
uvicorn app:app --reload --port 8000
```

---

## Docker

```bash
docker build -t rag-backend-slim .
docker run -p 8000:8000 rag-backend-slim
```

---

##  Kubernetes Deployment (Minikube)

###  Namespace

```bash
kubectl create namespace thinkwise-ai
```

### Apply Resources

```bash
kubectl apply -f k8s-deployment.yaml
```

---

## Deployment Architecture

```text
                    +----------------------+
                    |   Frontend (React)   |
                    +----------+-----------+
                               |
                     HTTP (Port 3000)
                               ↓
                +--------------------------+
                |   Backend (FastAPI App)  |
                | LangGraph ReAct Agent    |
                +-----------+--------------+
                            |
              REST (Port 11434) to Ollama LLM
                            ↓
                 +----------------------+
                 | Ollama LLM Container |
                 | Model: LLaMA 3       |
                 +----------------------+
```

All components are deployed as containers using **Minikube + Docker driver**.


## TODO

* MongoDB persistence for chat/idea history
* Fine-tuned scoring logic
* Better error handling

---
